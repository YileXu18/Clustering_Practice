# -*- coding: utf-8 -*-
"""homework-Team21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-B1vpp6D57b-8gyAZXtKeGuK2TnUb8V
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

from google.colab import drive
drive.mount('/content/drive')

Read_data = "/content/drive/MyDrive/BU_Clustering_Dataset_Final.csv"

Auto_data = pd.read_csv(Read_data)
Auto_data.info()
Auto_data.head()

"""## 1.

Below is the elbow plot we have, and it indicates that the optimal number of clusters should be 3. As we can see, after 3 clusters, with the increase in number of clusters, the within-cluster sum of squares started to decrease slowly. See figures in code.
"""

X = Auto_data[['LTV_DOLLARS', 'LTV_VISITS']]
wcss = []

# Try k values from 1 to 10
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the elbow plot
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Graph for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS value')
plt.show()

"""### 2.

In this question, we first drop column of ‘SITE’ and ‘UNIQUE_CUSTOMER_ID’. In is because 'UNIQUE_CUSTOMER_ID' is a unique identifier for each customer and
does not contribute to any pattern or group characteristics. The 'SITE' column, assuming it represents the website or location of engagement, does not have effect on the customer's clustering based on spending habits and demographics as well.

Then, we divided rest of columns into categorical and numerical data. We consider column of ‘MEMBER_SEGMENT’, ‘GIVEN_GENDER’, ‘AGE_RANGE’, and
‘INCOME_RANGE’ as category features since member segment represents different
types of members, given gender is considered as binary category, age range and income range both represents different range of people, they all considered as category. On the other hand, the rest are divided into numerical features, and we scale them into different feature types. See code for steps.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

#We first drop 'SITE' column and 'UNIQUE_CUSTOMER_ID' column
Auto_data_new = Auto_data.drop(['SITE', 'UNIQUE_CUSTOMER_ID'], axis=1)

gender_mapping = {'M': 0, 'F': 1}
member_segment_mapping = {'B-BIGSPENDER': 4, 'B-BUYER': 3, 'B-RETURNER': 2, 'B-BARGAINSHOPPER': 1}
age_mapping = {'18 - 24': 0, '25 - 34': 1,'35 - 44': 2,'45 - 54': 3, '55 - 64': 4,'65 - 74': 5, '75 +': 6}
income_mapping = {'Under $15 K': 0, '$15 - $19 K': 1, '$20 - $29 K': 2, '$30 - $39 K': 3, '$40 - $49 K': 4, '$50 - $74 K': 5,'$75 - $99 K': 6, '$100 - $124 K': 7, '$125+ K': 8}

Auto_data_new['GIVEN_GENDER'] = Auto_data_new['GIVEN_GENDER'].map(gender_mapping)
Auto_data_new['MEMBER_SEGMENT'] = Auto_data_new['MEMBER_SEGMENT'].map(member_segment_mapping)
Auto_data_new['AGE_RANGE'] = Auto_data_new['AGE_RANGE'].map(age_mapping)
Auto_data_new['INCOME_RANGE'] = Auto_data_new['INCOME_RANGE'].map(income_mapping)
Auto_data_new.head()

# Define the numerical columns we want to scale
numerical_cols = ['TENURE_DAYS', 'DAYS_SINCE_LAST_VISIT', 'DAYS_SINCE_FIRST_PURCHASE', 'DAYS_SINCE_LAST_PURCHASE', 'LTV_VISITS', 'LTV_ORDERS', 'LTV_DOLLARS', 'AVERAGE_ORDER_VALUE', 'DOLLARS_SAVED', 'PERCENT_DOLLARS_SAVED']
# Initialize the StandardScaler
scaler = StandardScaler()
# Fit and transform the numerical columns
Auto_data_new[numerical_cols] = scaler.fit_transform(Auto_data_new[numerical_cols])

missing_values = Auto_data_new.isnull().sum()
missing_values

X = Auto_data_new[['LTV_VISITS', 'LTV_DOLLARS']]

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create and fit the KMeans model
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X_scaled)

# Get cluster labels
cluster_labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Add cluster labels to your original dataset for visualization
Auto_data_new['Cluster'] = cluster_labels

# Plotting the clusters
plt.figure(figsize=(8, 6))
plt.scatter(Auto_data_new['LTV_VISITS'], Auto_data_new['LTV_DOLLARS'], c=Auto_data_new['Cluster'])
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', label='Centroids')
plt.title('K-means Clustering with k=3')
plt.xlabel('LTV_VISITS')
plt.ylabel('LTV_DOLLARS')
plt.legend()
plt.show()

"""## 3.

- Gender Distribution

• Cluster 1: Has a higher proportion of female customers (82.21%) compared to
male customers (17.79%).

• Cluster 2: Has an even higher proportion of female customers (88.95%) compared to male customers (11.05%).

• Cluster 3: Similar to the other clusters, it also has a higher proportion of female customers (83.33%) compared to male customers (16.67%).


- Income Distribution

• Cluster 1: It appears to have a broad distribution across income ranges, with the highest number of customers in the highest income bracket ($125+ K).

• Cluster 2: It has fewer members across all income brackets, but like Cluster 0, the highest income bracket is the most represented.

• Cluster 3: This cluster has very few members overall, but a significant proportion seems to be in the higher income brackets as well ($50 - $74 K and $125+ K).

- Average Metrics

• Cluster 1: This cluster has a mean LTV_DOLLARS score of about -0.25. This
indicates that this cluster's average LTV_DOLLARS is a quarter of a standard
deviation below the mean of the dataset. This could suggest they are either newer customers, or shop less frequently.

• Cluster 2: This cluster has a mean LTV_DOLLARS score of about 0.35. This
indicates that this cluster's average LTV_DOLLARS is a third of a standard
deviation above the mean of the dataset. This suggest that the customer from this cluster may shop slightly more frequently than customers in Cluster 1.

• Cluster 3: Cluster 3 has a mean LTV_DOLLARS score of about 4.65. This is
significantly above the mean, indicating that this cluster's average LTV_DOLLARS is 4.65 standard deviations above the dataset's mean. This is a substantial difference and highlights that this cluster is spending a lot more compared to the other clusters, suggesting that they are likely long-standing customers with frequent shopping visits or have higher ticket purchase.


- Cluster Comparisons and Characteristics

• Cluster 1 might represent the "regular" customer base with a wide income
range and more moderate spending habits.

• Cluster 2 could represent occasional shoppers or deal-seekers with lower
spending per visit and overall LTV.

• Cluster 3 is likely the "premium" segment, with few members but very high
spenders in terms of both LTV and order value. This cluster could be targeted
for luxury or high-value products.
"""

#Calculate proportions of male and female customers in each cluster
gender_distribution = Auto_data_new.groupby('Cluster')['GIVEN_GENDER'].value_counts(normalize=True) * 100
print(gender_distribution)
# Calculate income distribution in each cluster
income_distribution = Auto_data_new.groupby(['Cluster', 'INCOME_RANGE']).size().unstack()
print(income_distribution)
# Calculate average LTV dollars and average order value for each cluster
average_metrics = Auto_data_new.groupby('Cluster').agg({ 'LTV_DOLLARS': 'mean', 'AVERAGE_ORDER_VALUE': 'mean' })
# Print out the results print(gender_proportions)
print(average_metrics)

"""## 4.

Given the distinct characteristics of each cluster, a differentiated promotional strategy could be more effective than a one-size-fits-all approach. It is because for Cluster 1’s customer, the goal is to slightly increase transaction frequency or value. For Cluster 2 customer, the goal is to dive more significant purchases or increase purchase frequency. For Cluster 3 customer, as their purchase value and frequency are the highest, it is about
maintaining their high spending level and reinforcing brand loyalty. Hence we should not use the same promotional discount amounts for all. Here's how I would suggest tailoring the promotional discounts:

**Cluster 1: Regular Customers**

• **Strategy:** Offer a $10 off promotion.

This cluster has a lower-than-average LTV and seems to represent regular
customers with moderate spending habits. The aim here would be to increase
the frequency of their visits or the size of their average purchase without
significantly cutting into margins. Since this cluster's spending is not on the
high end, a smaller discount might be sufficient to encourage a higher
purchase frequency or slightly larger basket sizes.

**Cluster 2: Occasional Shoppers or Deal-Seekers**

• **Strategy:** Offer a $20 off promotion.

Customers in this cluster have a slightly higher LTV than Cluster 0 but a lower
average order value. They might be more price-sensitive and could be
attracted to a slightly higher discount. This could encourage them to
consolidate their purchases and spend more per visit. The higher discount
might also serve as an incentive to convert occasional shoppers into more
regular customers.

**Cluster 3: Premium Segment**

• **Strategy:**Offer a $30 off promotion.

This cluster includes the high spenders, with a significantly higher LTV and
average order value. They are less likely to be motivated by small discounts
due to their higher spending behavior. A $30 off promotion can be positioned
as an exclusive or premium offer, aligning with their purchasing patterns and
potentially increasing their loyalty. Additionally, given their high spending, the larger discount is less likely to affect the overall margins significantly.


By customizing the offers, the company can allocate marketing resources efficiently, maximizing the potential increase in revenue and customer satisfaction while maintaining profitability.
"""